{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# virtualenv check\n",
    "import sys\n",
    "\n",
    "def get_base_prefix_compat():\n",
    "    \"\"\"Get base/real prefix, or sys.prefix if there is none.\"\"\"\n",
    "    return getattr(sys, \"base_prefix\", None) or getattr(sys, \"real_prefix\", None) or sys.prefix\n",
    "\n",
    "def in_virtualenv():\n",
    "    return get_base_prefix_compat() != sys.prefix\n",
    "\n",
    "print(in_virtualenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greetings!\n",
    "\n",
    "## Simple Keyword Recognition on a google coral TPU\n",
    "\n",
    "This notebook serves as a simple showcase and tutorial on how to train a rudimentary speech-recognition neural network. Because there are already good guides on this topic, we'll add a little \"edge\" to it (heh) by venturing into the territory of microcontrollers and tpu. Here, size matters, for which tensorflow handily offers the tensorflow lite library. With functions to convert normal keras models to their lite versions, we'll see how this affects size and performance.\n",
    "\n",
    "To keep training times low and model size small, we'll focus on keywords (often times also called wakewords like \"Hey Siri\"), meaning short utterances like \"on\" and \"off\".\n",
    "\n",
    "Our final goal will be to deploy our model on a google coral tensor processing unit (tpu). As mentioned, for this project we'll use the tensorflow framework. This will also make the model creation, paramter searching and training a breeze. On top of that, tensorflow has strong deployment options.\n",
    "\n",
    "This notebook concerns only the dataset, neural network, training and testing pipelines and the conversion to lite. Another notebook will cover model deployment in an end-to-end system on a raspberry pi with connected tpu.\n",
    "\n",
    "### Content\n",
    "- Tensorflow Datasets\n",
    "- Preprocessing\n",
    "    - Feature Extraction\n",
    "    - Input Pipeline\n",
    "- Neural Network\n",
    "- Training\n",
    "- Testing\n",
    "- Tf-Lite Conversion and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Datasets\n",
    "\n",
    "Tensorflow datasets (tfds) are a quick way of acquiring data. The dataset object comes with a multitude of methods to transform the data and serve it to the model. It also conviently displays info about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"speech_commands\"\n",
    "\n",
    "\n",
    "speech_builder = tfds.builder(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtzan_music_speech\n",
      "librispeech\n",
      "librispeech_lm\n",
      "ljspeech\n",
      "speech_commands\n",
      "tfds.core.DatasetInfo(\n",
      "    name='speech_commands',\n",
      "    version=0.0.2,\n",
      "    description='An audio dataset of spoken words designed to help train and evaluate keyword\n",
      "spotting systems. Its primary goal is to provide a way to build and test small\n",
      "models that detect when a single word is spoken, from a set of ten target words,\n",
      "with as few false positives as possible from background noise or unrelated\n",
      "speech. Note that in the train and validation set, the label \"unknown\" is much\n",
      "more prevalent than the labels of the target words or background noise.\n",
      "One difference from the release version is the handling of silent segments.\n",
      "While in the test set the silence segments are regular 1 second files, in the\n",
      "training they are provided as long segments under \"background_noise\" folder.\n",
      "Here we split these background noise into 1 second clips, and also keep one of\n",
      "the files for the validation set.',\n",
      "    homepage='https://arxiv.org/abs/1804.03209',\n",
      "    features=FeaturesDict({\n",
      "        'audio': Audio(shape=(None,), dtype=tf.int64),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=12),\n",
      "    }),\n",
      "    total_num_examples=100503,\n",
      "    splits={\n",
      "        'test': 4890,\n",
      "        'train': 85511,\n",
      "        'validation': 10102,\n",
      "    },\n",
      "    supervised_keys=('audio', 'label'),\n",
      "    citation=\"\"\"@article{speechcommandsv2,\n",
      "       author = {{Warden}, P.},\n",
      "        title = \"{Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition}\",\n",
      "      journal = {ArXiv e-prints},\n",
      "      archivePrefix = \"arXiv\",\n",
      "      eprint = {1804.03209},\n",
      "      primaryClass = \"cs.CL\",\n",
      "      keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},\n",
      "        year = 2018,\n",
      "        month = apr,\n",
      "        url = {https://arxiv.org/abs/1804.03209},\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"!!! DOWNLOAD WARNING !!!\n",
    "\n",
    "This cells downloads the speech commands dataset. Will not download twice, if it detects an already\n",
    "downloaded version.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# looking for speech commands dataset in all available datasets...\n",
    "for ele in tfds.list_builders():\n",
    "    if \"speech\" in ele:\n",
    "        print(ele)\n",
    "\n",
    "dataset_name = \"speech_commands\"\n",
    "\n",
    "# instantiate a dataset builder (see tensorflow dataset builder)\n",
    "speech_builder = tfds.builder(dataset_name)\n",
    "print(speech_builder.info)\n",
    "\n",
    "# download data into existing data folder\n",
    "speech_builder.download_and_prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are:\n",
      "['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes', '_silence_', '_unknown_']\n",
      "\n",
      "Lets look directly at the first element of the dataset.\n",
      "Because it is a <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>,we need to iterate through it in order to actually load elements.\n",
      "\n",
      "The dataset consists of:  <class 'tuple'>\n",
      "How does a label object look like:  tf.Tensor(7, shape=(), dtype=int64)\n",
      "How does a audio object look like:  tf.Tensor([  -1   -2   -2 ... -136 -170 -203], shape=(16000,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get some info from the builder about the dataset\"\"\"\n",
    "builder_info = speech_builder.info\n",
    "num_labels = builder_info.features['label'].num_classes\n",
    "label_list = builder_info.features['label'].names\n",
    "\n",
    "\"\"\"Not we actually acquire the dataset object. As supervised gives us a dataset of tuples: data and label\"\"\"\n",
    "data = speech_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "assert isinstance(data, dict)\n",
    "\n",
    "print(\"Labels are:\\n{}\\n\".format(label_list))\n",
    "\n",
    "test_set = data['test']\n",
    "train_set = data['train']\n",
    "validation_set = data['validation']\n",
    "\n",
    "# I've got these numbers by simple iterating through the datasets, but I'm sure there's a more elegant way\n",
    "test_set_size = 4890\n",
    "train_set_size = 85511\n",
    "validation_set_size = 10102\n",
    "\n",
    "print(\"Lets look directly at the first element of the dataset.\\nBecause it is a {},\\\n",
    "we need to iterate through it in order to actually load elements.\\n\".format(type(test_set)))\n",
    "for thing in test_set:\n",
    "    print(\"The dataset consists of: \", type(thing))\n",
    "    \n",
    "    print(\"How does a label object look like: \", thing[1])\n",
    "    print(\"How does a audio object look like: \", thing[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ents(dataset):\n",
    "    \"\"\"Counts entities of a given dataset, returns them as dict(labels, count)\"\"\"\n",
    "    result = {}\n",
    "    for thing in dataset:\n",
    "        label = thing[1].numpy()\n",
    "        #print(label)\n",
    "        #break\n",
    "        if(label in result):\n",
    "            result[label] += 1\n",
    "        else:\n",
    "            result[label] = 1\n",
    "    return result\n",
    "\n",
    "def make_plot(entity_count_dict, label_list):\n",
    "    \"\"\"Converts a dict of entity counts to a list, then plots them with the corresponded labels\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    # make a distribution\n",
    "    dist = [entity_count_dict[x] for x in range(len(label_list))]\n",
    "    ax.bar(label_list, dist)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Get the distributions of commands as a dictionary\"\"\"\n",
    "train_dist = count_ents(train_set)\n",
    "test_dist = count_ents(test_set)\n",
    "validation_dist = count_ents(validation_set)\n",
    "\n",
    "langs = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes', '_silence_', '_unknown_']\n",
    "\n",
    "\"\"\"Plot the data\"\"\"\n",
    "make_plot(train_dist, langs)\n",
    "make_plot(test_dist, langs)\n",
    "make_plot(validation_dist, langs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for the validaiton and training sets, \"unknown\" labeled files are present in a much higher number than the desired keywords. This of course makes sense for a keyword recognition system, where most oft the times speech unrelated to our commands is spoken. A system needs to confidently recognize unrelated speech. \n",
    "\n",
    "In order to make difference in the actual commands more visible, we now exclude the \"unknown\" label from our plot and arrive at a better representation of our data. We see that all words have a rather same frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "langs = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes', '_silence_']\n",
    "\n",
    "make_plot(train_dist, langs)\n",
    "make_plot(test_dist, langs)\n",
    "make_plot(validation_dist, langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before fitting any model, we need to look at our data and preprocess it, in order to make it actually usuable.\n",
    "\n",
    "### Feature Extraction\n",
    "#### Data\n",
    "While raw visual data (eg. rgb pictures) already have enough information for a model to learn from, working with audio requires an additional step: feature extraction. There are attempts to use raw audio data in deep neural networks, but current state of the art systems for speech recognition often use specialized features that are computed from the raw audio signal: mel frequency cepstral coefficents. \n",
    "\n",
    "There are audio libaries like librosa that make the extraction rather easy, but because we're working with tensorflow Datasets it's best to stay in its context (otherwise we would've to jump a couple of hurdles because of the way tfds and especially PrefetchDatasets work and trust me, we dont want to do that here...). We find an example how to extract mfcc working with tensorflow tools in the official documentation. Simply pack it into a function (fingers crossed it's working correctly) for later use.\n",
    "\n",
    "\n",
    "#### Labels\n",
    "Converting to one hot encodings is only needed when we dont want to use sparse categorical entropy loss,\n",
    "We further need to convert the labels which are currently simply integers to a one-hot representation. Again, tensorflow already offers a neat function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mfccs(audio, labels):\n",
    "    \n",
    "    FRAME_RATE = 16000\n",
    "    \n",
    "    stfts = tf.signal.stft(tf.cast(audio, tf.float32), frame_length=1024, frame_step=256,\n",
    "                       fft_length=1024)\n",
    "    \n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins,\n",
    "                                                                        FRAME_RATE, lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    \n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    \n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    # You can use other parts, or even all of the MFCCs, to test around how it affect the accuracy\n",
    "    mfcc = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
    "    \n",
    "    # Finally add depth to the mfcc tensors, our Conv2D-Model requires this.\n",
    "    mfcc = tf.expand_dims(mfcc, -1)\n",
    "    \n",
    "    return mfcc, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline\n",
    "\n",
    "While we now have a method to extract mfccs, we need to incorporate it into our input- or data-pipeline. This describes the process of how we feed our data into the model. We want to only work on batches on data (size of a batch can influence model learning) and of course extract our features. We also want to shuffle the data, improving generalization of out model.\n",
    "\n",
    "We can do all this by directly calling methods of our dataset. While it takes a little time getting used to and finding the correct parameters, the result is clean and straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_set.map(make_mfccs\n",
    "                             ).shuffle(train_set_size\n",
    "                             ).padded_batch(128\n",
    "                             ).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = test_set.map(make_mfccs\n",
    "                           ).padded_batch(128\n",
    "                           ).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_validation = validation_set.map(make_mfccs\n",
    "                                       ).padded_batch(128\n",
    "                                       ).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Because we're limited through our use of tensorflow lite, but also because we want to keep it simple, we use a rather simple composition of layers for our neural network. \n",
    "\n",
    "Following the standards, we use 2D-convolutional layers to highlight important features. As regulations batchnormalization layers are used and rectified liniear units as activation functions. Finally a pooling layer builds the end of one layer-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 57, 11, 128)       1280      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 55, 9, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 55, 9, 128)        512       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 55, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 27, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 3, 64)         32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 2, 64)         16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25, 2, 64)         256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 25, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                780       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 199,692\n",
      "Trainable params: 199,308\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, GlobalAveragePooling2D, Dense, Softmax\n",
    "from kapre.composed import get_melspectrogram_layer, get_log_frequency_spectrogram_layer\n",
    "\n",
    "input_shape = (59,13,1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3), strides=1, input_shape=input_shape))\n",
    "model.add(Conv2D(128, (3), strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ReLU())\n",
    "model.add(MaxPooling2D(pool_size=2, padding=\"valid\"))\n",
    "\n",
    "model.add(Conv2D(64, 2, strides=1))\n",
    "model.add(Conv2D(64, 2, strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ReLU())\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2), padding=\"valid\"))\n",
    "\n",
    "#model.add(Conv2D(6, (2, 2), strides=(2, 2)))\n",
    "#model.add(Conv2D(16, (2, 2), strides=(2, 2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(ReLU())\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "#model.add(Dense(1))\n",
    "model.add(Softmax())\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')#, run_eagerly=False)#'categorical_crossentropy', 'mse'\n",
    "model.build()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally we arrive at the training. Because we already did our batching in the dataset-object, we don't really have much else to do here. Of course we still follow standard procedure by using callbacks to stop the training if nothing improved during a period of 10 epochs and saving the model with the best weights.\n",
    "\n",
    "We could, however, further improve training by using a learn rate adaption technique to handle local minimas, adjust the learning rate during training (giving it velocity) and ... RESEARCH HERE\n",
    "\n",
    "It is also a good idea to use something like gridsearch to automate parameter testing, which is however heavily memory expensive.\n",
    "\n",
    "For now, we're content with a simple training loop.\n",
    "\n",
    "Becaus we currently use the simplest way of utilizing tensorflow datasets with a keras model, validation while training is not possible. We would need to build a iterator generator for the dataset. Maybe I'll implement it later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 1.0389\n",
      "Epoch 00001: loss improved from inf to 1.03885, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 263s 394ms/step - loss: 1.0388\n",
      "Epoch 2/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.5186\n",
      "Epoch 00002: loss improved from 1.03885 to 0.51862, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 246s 367ms/step - loss: 0.5186\n",
      "Epoch 3/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.3886\n",
      "Epoch 00003: loss improved from 0.51862 to 0.38867, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 201s 301ms/step - loss: 0.3887\n",
      "Epoch 4/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.3317\n",
      "Epoch 00004: loss improved from 0.38867 to 0.33176, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 193s 288ms/step - loss: 0.3318\n",
      "Epoch 5/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2980\n",
      "Epoch 00005: loss improved from 0.33176 to 0.29799, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 205s 307ms/step - loss: 0.2980\n",
      "Epoch 6/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2716\n",
      "Epoch 00006: loss improved from 0.29799 to 0.27160, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 190s 285ms/step - loss: 0.2716\n",
      "Epoch 7/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2546\n",
      "Epoch 00007: loss improved from 0.27160 to 0.25455, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 179s 268ms/step - loss: 0.2546\n",
      "Epoch 8/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2412\n",
      "Epoch 00008: loss improved from 0.25455 to 0.24117, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 179s 268ms/step - loss: 0.2412\n",
      "Epoch 9/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00009: loss improved from 0.24117 to 0.22793, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 177s 264ms/step - loss: 0.2279\n",
      "Epoch 10/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00010: loss improved from 0.22793 to 0.22015, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 180s 270ms/step - loss: 0.2202\n",
      "Epoch 11/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2083\n",
      "Epoch 00011: loss improved from 0.22015 to 0.20829, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 177s 265ms/step - loss: 0.2083\n",
      "Epoch 12/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00012: loss improved from 0.20829 to 0.20253, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 177s 265ms/step - loss: 0.2025\n",
      "Epoch 13/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1945\n",
      "Epoch 00013: loss improved from 0.20253 to 0.19448, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 178s 265ms/step - loss: 0.1945\n",
      "Epoch 14/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00014: loss improved from 0.19448 to 0.18909, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 185s 276ms/step - loss: 0.1891\n",
      "Epoch 15/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1811\n",
      "Epoch 00015: loss improved from 0.18909 to 0.18112, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 184s 275ms/step - loss: 0.1811\n",
      "Epoch 16/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1776\n",
      "Epoch 00016: loss improved from 0.18112 to 0.17762, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 207s 310ms/step - loss: 0.1776\n",
      "Epoch 17/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1729\n",
      "Epoch 00017: loss improved from 0.17762 to 0.17286, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 205s 306ms/step - loss: 0.1729\n",
      "Epoch 18/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1681\n",
      "Epoch 00018: loss improved from 0.17286 to 0.16804, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 204s 305ms/step - loss: 0.1680\n",
      "Epoch 19/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1632\n",
      "Epoch 00019: loss improved from 0.16804 to 0.16323, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 206s 308ms/step - loss: 0.1632\n",
      "Epoch 20/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1613\n",
      "Epoch 00020: loss improved from 0.16323 to 0.16127, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 202s 302ms/step - loss: 0.1613\n",
      "Epoch 21/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1571\n",
      "Epoch 00021: loss improved from 0.16127 to 0.15708, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 201s 300ms/step - loss: 0.1571\n",
      "Epoch 22/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1510\n",
      "Epoch 00022: loss improved from 0.15708 to 0.15099, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 198s 296ms/step - loss: 0.1510\n",
      "Epoch 23/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1477\n",
      "Epoch 00023: loss improved from 0.15099 to 0.14783, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 184s 275ms/step - loss: 0.1478\n",
      "Epoch 24/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1497\n",
      "Epoch 00024: loss did not improve from 0.14783\n",
      "669/669 [==============================] - 190s 284ms/step - loss: 0.1497\n",
      "Epoch 25/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1414\n",
      "Epoch 00025: loss improved from 0.14783 to 0.14144, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 187s 279ms/step - loss: 0.1414\n",
      "Epoch 26/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1391\n",
      "Epoch 00026: loss improved from 0.14144 to 0.13911, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 181s 270ms/step - loss: 0.1391\n",
      "Epoch 27/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00027: loss improved from 0.13911 to 0.13639, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 184s 275ms/step - loss: 0.1364\n",
      "Epoch 28/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1345\n",
      "Epoch 00028: loss improved from 0.13639 to 0.13455, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 187s 279ms/step - loss: 0.1346\n",
      "Epoch 29/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1332\n",
      "Epoch 00029: loss improved from 0.13455 to 0.13322, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 187s 279ms/step - loss: 0.1332\n",
      "Epoch 30/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00030: loss improved from 0.13322 to 0.12910, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 182s 272ms/step - loss: 0.1291\n",
      "Epoch 31/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00031: loss improved from 0.12910 to 0.12632, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 179s 267ms/step - loss: 0.1263\n",
      "Epoch 32/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00032: loss improved from 0.12632 to 0.12339, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 191s 285ms/step - loss: 0.1234\n",
      "Epoch 33/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00033: loss improved from 0.12339 to 0.12174, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 189s 282ms/step - loss: 0.1217\n",
      "Epoch 34/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00034: loss improved from 0.12174 to 0.11884, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 191s 285ms/step - loss: 0.1188\n",
      "Epoch 35/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00035: loss improved from 0.11884 to 0.11727, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 197s 295ms/step - loss: 0.1173\n",
      "Epoch 36/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00036: loss improved from 0.11727 to 0.11676, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 206s 307ms/step - loss: 0.1168\n",
      "Epoch 37/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00037: loss improved from 0.11676 to 0.11561, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 192s 286ms/step - loss: 0.1156\n",
      "Epoch 38/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00038: loss improved from 0.11561 to 0.11107, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 197s 295ms/step - loss: 0.1111\n",
      "Epoch 39/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00039: loss improved from 0.11107 to 0.10897, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 181s 271ms/step - loss: 0.1090\n",
      "Epoch 40/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1060\n",
      "Epoch 00040: loss improved from 0.10897 to 0.10599, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 195s 291ms/step - loss: 0.1060\n",
      "Epoch 41/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1046\n",
      "Epoch 00041: loss improved from 0.10599 to 0.10461, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 204s 305ms/step - loss: 0.1046\n",
      "Epoch 42/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1039\n",
      "Epoch 00042: loss improved from 0.10461 to 0.10393, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 205s 307ms/step - loss: 0.1039\n",
      "Epoch 43/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00043: loss improved from 0.10393 to 0.10057, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 210s 314ms/step - loss: 0.1006\n",
      "Epoch 44/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00044: loss improved from 0.10057 to 0.09875, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 193s 288ms/step - loss: 0.0988\n",
      "Epoch 45/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0989\n",
      "Epoch 00045: loss did not improve from 0.09875\n",
      "669/669 [==============================] - 195s 292ms/step - loss: 0.0989\n",
      "Epoch 46/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0960\n",
      "Epoch 00046: loss improved from 0.09875 to 0.09595, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 207s 309ms/step - loss: 0.0960\n",
      "Epoch 47/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0942\n",
      "Epoch 00047: loss improved from 0.09595 to 0.09420, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 186s 279ms/step - loss: 0.0942\n",
      "Epoch 48/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0919\n",
      "Epoch 00048: loss improved from 0.09420 to 0.09186, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 206s 308ms/step - loss: 0.0919\n",
      "Epoch 49/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0921\n",
      "Epoch 00049: loss did not improve from 0.09186\n",
      "669/669 [==============================] - 198s 295ms/step - loss: 0.0921\n",
      "Epoch 50/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0889\n",
      "Epoch 00050: loss improved from 0.09186 to 0.08891, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 193s 288ms/step - loss: 0.0889\n",
      "Epoch 51/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0869\n",
      "Epoch 00051: loss improved from 0.08891 to 0.08693, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 191s 286ms/step - loss: 0.0869\n",
      "Epoch 52/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0862\n",
      "Epoch 00052: loss improved from 0.08693 to 0.08619, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 188s 281ms/step - loss: 0.0862\n",
      "Epoch 53/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0859\n",
      "Epoch 00053: loss improved from 0.08619 to 0.08585, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 209s 313ms/step - loss: 0.0858\n",
      "Epoch 54/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0845\n",
      "Epoch 00054: loss improved from 0.08585 to 0.08455, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 198s 295ms/step - loss: 0.0846\n",
      "Epoch 55/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0919\n",
      "Epoch 00055: loss did not improve from 0.08455\n",
      "669/669 [==============================] - 188s 281ms/step - loss: 0.0919\n",
      "Epoch 56/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0793\n",
      "Epoch 00056: loss improved from 0.08455 to 0.07934, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 185s 277ms/step - loss: 0.0793\n",
      "Epoch 57/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0780\n",
      "Epoch 00057: loss improved from 0.07934 to 0.07795, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 189s 283ms/step - loss: 0.0779\n",
      "Epoch 58/200\n",
      "668/669 [============================>.] - ETA: 0s - loss: 0.0778\n",
      "Epoch 00058: loss improved from 0.07795 to 0.07783, saving model to model_4.hdf5\n",
      "669/669 [==============================] - 184s 275ms/step - loss: 0.0778\n",
      "Epoch 59/200\n",
      "538/669 [=======================>......] - ETA: 38s - loss: 0.0770"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-35059b6937f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             save_weights_only=False, mode=\"min\",save_freq=\"epoch\",options=None)\n\u001b[1;32m     10\u001b[0m             ]\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# eine epoche besteht hier nur aus einem batch! wirklich?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/keywords2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# better to use iterator generators\n",
    "#model.fit(dataset_1, validation_data=dataset_validation, validation_batch_size=128, epochs=40)\n",
    "model_nr = str(4)\n",
    "model_path = \"model_\"+model_nr+\".hdf5\"\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\", restore_best_weights=True),\n",
    "            ModelCheckpoint(model_path, monitor=\"loss\", verbose=1, save_best_only=True,\n",
    "                            save_weights_only=False, mode=\"min\",save_freq=\"epoch\",options=None)\n",
    "            ]\n",
    "model.fit(dataset_train, epochs=200, callbacks=callbacks)\n",
    "\n",
    "# eine epoche besteht hier nur aus einem batch! wirklich?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Time of truth, ladies and gents, we're now using our model to predict the words of previously unseen audio!\n",
    "\n",
    "Simple as it is, we feed our test and validation set into the model and compare its output to the actual labels, voila! Not much else to say here.\n",
    "\n",
    "Maybe a couple words on the deployment. Following lines of code would be the bedrock of a keyword recognition software. Of course we still have to preprocess the data, but the rest of the main logic relies on these few lines of code. What to do if a specific keyword is recognized, well thats a question for another time (or use case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_acc(predictions, dataset):\n",
    "    total, correct = 0, 0\n",
    "    index = 0\n",
    "\n",
    "    for batch in dataset:\n",
    "        for b_ele in batch[1]:\n",
    "            #print(b_ele)\n",
    "            total += 1\n",
    "            if b_ele == np.argmax(predictions[index]):\n",
    "                correct += 1\n",
    "            index += 1\n",
    "\n",
    "\n",
    "    print(\"{} out of {} correct, resulting in an accuracy of {}\\n\".format(correct, total, (correct/total)*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 44s 551ms/step\n",
      "39/39 [==============================] - 21s 530ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_val = model.predict(dataset_validation, verbose=1)\n",
    "preds_test = model.predict(dataset_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip of the day:\n",
    "\n",
    "It's always a good idea to make three checks when working with data in general:\n",
    "- What data type are you getting?\n",
    "- What data shape are you getting?\n",
    "- What data are you actually getting?\n",
    "\n",
    "Always checking these three, even if you think you know what you've got, can save you alot of trouble and reassure your way of thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10102, 12)\n",
      "[3.5163629e-04 2.6183114e-03 1.8488441e-01 8.0643678e-01 1.5516784e-05\n",
      " 1.1606327e-05 2.6408462e-03 6.4246313e-05 1.4864680e-03 9.9873240e-04\n",
      " 1.1137000e-06 4.9026223e-04]\n"
     ]
    }
   ],
   "source": [
    "print(type(preds_val))\n",
    "print(preds_val.shape)\n",
    "print(preds_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9415 out of 10102 correct, resulting in an accuracy of 93.19936646208672\n",
      "\n",
      "4146 out of 4890 correct, resulting in an accuracy of 84.78527607361963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calc_acc(preds_val, dataset_validation)\n",
    "calc_acc(preds_test, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 out of 4890 correct, resulting in an accuracy of 8.302658486707566\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for target in y_test_trans:\n",
    "    total += 1\n",
    "    pred_idx = random.randrange(0, 11)\n",
    "    #print(pred_idx)\n",
    "    if target[pred_idx]==1:\n",
    "        correct += 1\n",
    "\n",
    "print(\"{} out of {} correct, resulting in an accuracy of {}\".format(correct, total, (correct/total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Further train the model with validaton set \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Lite Conversion and Quantization\n",
    "\n",
    "We're now in the postition to convert our model to tflite format, which greatly reduces its size. We also want to quantize it, bringing the weights from float32 to int16. This comes with a reduction in accuracy, but needs to be done if we want to run the model on an tpu, like google corals tpu-accelerator.\n",
    "\n",
    "#### A Note:\n",
    "We're using post-training quantization, which reduces the precision of a models weights from float to int. While this is quick, the model potentially loses a good amout of accuracy. This can be combated by using a quantization aware training approach. Here, fake quantization nodes are introduced which help the model to compensate for the information loss of quantization. \n",
    "\n",
    "Quantization aware training usually increases the training time, which is why we refrained from it. It would be  easy to implement nonetheless, using following lines before the training process.\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "qat_model = tfmot.quantization.keras.quantize_model(your_keras_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpj3oj6969/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpj3oj6969/assets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tensorflow lite converter\"\"\"\n",
    "\n",
    "#\n",
    "#\n",
    "#ToDo: \n",
    "# Full interger quantization\n",
    "#https://www.tensorflow.org/lite/performance/post_training_integer_quant?hl=en#convert_using_integer-only_quantization\n",
    "#https://www.tensorflow.org/lite/performance/post_training_quantization?hl=en\n",
    "#\n",
    "# useful link: https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca\n",
    "\n",
    "import tensorflow.lite as lite\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('model_4.hdf5')\n",
    "\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "# This\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpn77kplom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/mx/q84y7s0d107182d1vknn_rmh0000gn/T/tmpn77kplom/assets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "But wait! We only converted weights to int, but not the input and output nodes! \n",
    "We need some additional limes for this...\n",
    "We need a representative dataset because it \"(..) can be used to evaluate optimizations by the converter. \n",
    "E.g. converter can use these examples to estimate (min, max) ranges by calibrating the model on inputs.\" Yep.\n",
    "\"\"\"\n",
    "import tensorflow.lite as lite\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('model_4.hdf5')\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in dataset_test.take(100):\n",
    "        # Remember, we get a tuple of (audio, label) tensors, also we added an additional dimension to it, so...\n",
    "        #in_val = tf.squeeze(input_value[0], axis=0)\n",
    "        #print(type(in_val), in_val.shape)\n",
    "        yield [input_value[0]]\n",
    "        #yield [in_val]\n",
    "        \n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# \n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model. TODO: print size of normal and lite model\n",
    "with open('model_full_ints.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keywords2",
   "language": "python",
   "name": "keywords2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
