{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# virtualenv check\n",
    "import sys\n",
    "\n",
    "def get_base_prefix_compat():\n",
    "    \"\"\"Get base/real prefix, or sys.prefix if there is none.\"\"\"\n",
    "    return getattr(sys, \"base_prefix\", None) or getattr(sys, \"real_prefix\", None) or sys.prefix\n",
    "\n",
    "def in_virtualenv():\n",
    "    return get_base_prefix_compat() != sys.prefix\n",
    "\n",
    "print(in_virtualenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greetings!\n",
    "\n",
    "## Simple Keyword Recognition on a google coral TPU\n",
    "\n",
    "This notebook is a little showcase and tutorial on how to train a simple speech-recognition model through deep learning. To keep training times low and model size small, we'll focus on keywords (a wakeword an some commands like \"on\", \"off\", etc. \n",
    "\n",
    "Because we want to deploy our model on google's tpu, we'll use tensorflow and tensorflow lite. This will also make the creation, paramter searching and training a breeze. On top of that, tensorflow has strong deployment options.\n",
    "\n",
    "This notebook concerns only the neural network. Another one will cover model deployment in an end-to-end system on a raspberry pi with connected tpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.lite as lite\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pydub\n",
    "import kapre\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"speech_commands\"\n",
    "\n",
    "\n",
    "speech_builder = tfds.builder(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!!! DOWNLOAD WARNING !!!\"\"\"\n",
    "\n",
    "# looking for speech commands dataset in all available datasets...\n",
    "for ele in tfds.list_builders():\n",
    "    if \"speech\" in ele:\n",
    "        print(ele)\n",
    "\n",
    "dataset_name = \"speech_commands\"\n",
    "\n",
    "# instantiate a dataset builder (see tensorflow dataset builder)\n",
    "speech_builder = tfds.builder(dataset_name)\n",
    "print(speech_builder.info)\n",
    "\n",
    "# download data into existing data folder\n",
    "speech_builder.download_and_prepare(download_dir=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ClassLabel' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e337814083ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuilder_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ClassLabel' object is not iterable"
     ]
    }
   ],
   "source": [
    "builder_info = speech_builder.info\n",
    "num_labels = builder_info.features['label'].num_classes\n",
    "print(num_labels)\n",
    "for x in builder_info.features['label']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'train', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "data = speech_builder.as_dataset()\n",
    "assert isinstance(data, dict)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "audio : \n",
      "tf.Tensor([  -1   -2   -2 ... -136 -170 -203], shape=(16000,), dtype=int64)\n",
      "label : \n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "#############\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for dic in data['test']:\n",
    "    print(type(dic))\n",
    "    for key, val in dic.items():\n",
    "        print(key, \": \")\n",
    "        print(val)\n",
    "        #if key == 'audio'\n",
    "        #output_signal = tf.audio.encode_wav(val, input_signal[1])\n",
    "    print('#############')\n",
    "    i+=1\n",
    "    if i==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in data['test']:\n",
    "    audio_tensor = dic['audio']\n",
    "    label_tensor = dic['label']\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Build\n",
    "\n",
    "Next, we build a standard Convolutional Neural Network with a little twist. Instead of doing feature extraction beforehand, in a preprocessing-pipeline, we'll use the specialized layers provided by kapre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense, Softmax\n",
    "from kapre.composed import get_melspectrogram_layer, get_log_frequency_spectrogram_layer\n",
    "\n",
    "input_shape = (16000,1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "melgram_layer = get_melspectrogram_layer(input_shape=input_shape, n_fft=2048, win_length=2018, hop_length=1024,\n",
    "                                        input_data_format='channels_last', output_data_format='channels_last',\n",
    "                                        sample_rate=16000, name='melspectro_layer')\n",
    "model.add(melgram_layer)\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), strides=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ReLU())\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Softmax())\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', 'categorical_crossentropy', run_eagerly=False)#'categorical_crossentropy', 'mse'\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "melspectro_layer (Sequential (None, 14, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 6, 63, 32)         320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6, 63, 32)         128       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 6, 63, 32)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 844\n",
      "Trainable params: 780\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85510"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "for i,x in enumerate(train_dataset):\n",
    "    \n",
    "    y = i\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data[\"train\"]\n",
    "train_dataset = tfds.as_numpy(train_dataset) \n",
    "#print(train_dataset)\n",
    "x_train = np.zeros((85511, 16000))\n",
    "y_train = np.empty((85511))\n",
    "\n",
    "for i, dic in enumerate(train_dataset):\n",
    "    \n",
    "    # seperate the x and y\n",
    "    x = dic[\"audio\"]\n",
    "    x_train[i][:x.shape[0]] = x\n",
    "    y_train[i] = dic[\"label\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_trans = tf.keras.utils.to_categorical(y_train, num_classes=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "669/669 [==============================] - 97s 145ms/step - loss: 1.5804\n",
      "Epoch 2/40\n",
      "669/669 [==============================] - 95s 143ms/step - loss: 1.5028\n",
      "Epoch 3/40\n",
      "669/669 [==============================] - 93s 140ms/step - loss: 1.4894\n",
      "Epoch 4/40\n",
      "124/669 [====>.........................] - ETA: 1:14 - loss: 1.4926"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train_trans, batch_size=128, epochs=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keywords2",
   "language": "python",
   "name": "keywords2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
